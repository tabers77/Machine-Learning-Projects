{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "\n",
    "- Try to put everything in google colab\n",
    "\n",
    "**EDA**\n",
    "\n",
    "- add findings \n",
    "\n",
    "**SEELCTION**\n",
    "\n",
    "- skip radom forest for the moment\n",
    "- run bayesian opt...\n",
    "- Nueral nets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- https://github.com/krishnaik06/NYC-Taxi-Fares-Prediction \n",
    "- https://www.kaggle.com/madhurisivalenka/cleansing-eda-modelling-lgbm-xgboost-starters\n",
    "- https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependecies\n",
    "\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "\n",
    "# Set a standard size and style\n",
    "sns.set_style('whitegrid') \n",
    "rcParams['figure.figsize'] = (6, 4) # define plot sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take a sample of our dataset\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/krishnaik06/NYC-Taxi-Fares-Prediction/master/taxifare.csv'\n",
    "\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>fare_class</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-04-19 08:17:56 UTC</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.992365</td>\n",
       "      <td>40.730521</td>\n",
       "      <td>-73.975499</td>\n",
       "      <td>40.744746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-04-17 15:43:53 UTC</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990078</td>\n",
       "      <td>40.740558</td>\n",
       "      <td>-73.974232</td>\n",
       "      <td>40.744114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-04-17 11:23:26 UTC</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994149</td>\n",
       "      <td>40.751118</td>\n",
       "      <td>-73.960064</td>\n",
       "      <td>40.766235</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-04-11 21:25:03 UTC</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-73.990485</td>\n",
       "      <td>40.756422</td>\n",
       "      <td>-73.971205</td>\n",
       "      <td>40.748192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-17 02:19:01 UTC</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.990976</td>\n",
       "      <td>40.734202</td>\n",
       "      <td>-73.905956</td>\n",
       "      <td>40.743115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pickup_datetime  fare_amount  fare_class  pickup_longitude  \\\n",
       "0  2010-04-19 08:17:56 UTC          6.5           0        -73.992365   \n",
       "1  2010-04-17 15:43:53 UTC          6.9           0        -73.990078   \n",
       "2  2010-04-17 11:23:26 UTC         10.1           1        -73.994149   \n",
       "3  2010-04-11 21:25:03 UTC          8.9           0        -73.990485   \n",
       "4  2010-04-17 02:19:01 UTC         19.7           1        -73.990976   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.730521         -73.975499         40.744746                1  \n",
       "1        40.740558         -73.974232         40.744114                1  \n",
       "2        40.751118         -73.960064         40.766235                2  \n",
       "3        40.756422         -73.971205         40.748192                1  \n",
       "4        40.734202         -73.905956         40.743115                1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My utc is 5  hours ahead NY and therefore I substract 5 hours \n",
    "df = data.copy()\n",
    "\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime']) - datetime.timedelta(hours=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generat new date features\n",
    "df['Month']=df['pickup_datetime'].dt.month\n",
    "df['Day']=df['pickup_datetime'].dt.day\n",
    "df['Hours']=df['pickup_datetime'].dt.hour\n",
    "df['Minutes']=df['pickup_datetime'].dt.minute\n",
    "df['Day of week']=df['pickup_datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column to specify morning and night \n",
    "df['morning_night']=np.where(df['Hours']<12,0,1)\n",
    "\n",
    "# Drop pickup datetime\n",
    "df.drop('pickup_datetime',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Haversine distances \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Haversine distances \n",
    "\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians\n",
    "newdelhi = [28.6139, 77.2090]\n",
    "bangalore = [12.9716, 77.5946]\n",
    "\n",
    "newdelhi_in_radians = [radians(_) for _ in newdelhi]\n",
    "bangalore_in_radians = [radians(_) for _ in bangalore]\n",
    "\n",
    "result = haversine_distances([newdelhi_in_radians, bangalore_in_radians])\n",
    "result*6371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.radians(df['dropoff_latitude']-df[\"pickup_latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calcualte haversine distance \n",
    "def haversine(df):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    lat1= np.radians(df[\"pickup_latitude\"])\n",
    "    lat2 = np.radians(df[\"dropoff_latitude\"])\n",
    "    #### Based on the formula  x1=drop_lat,x2=dropoff_long \n",
    "    dlat = np.radians(df['dropoff_latitude']-df[\"pickup_latitude\"])\n",
    "    dlong = np.radians(df[\"dropoff_longitude\"]-df[\"pickup_longitude\"])\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlong/2)**2\n",
    "\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_distance']=haversine(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He used PCA to transform longitude and latitude coordinates. In this case it is not about dimension reduction since he transformed 2D-> 2D. The rotation could help for decision tree splits, and it did actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.vstack((df[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    df[['dropoff_latitude', 'dropoff_longitude']].values\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "pca = PCA().fit(coords)\n",
    "df['pickup_pca0'] = pca.transform(df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "df['pickup_pca1'] = pca.transform(df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "df['dropoff_pca0'] = pca.transform(df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "df['dropoff_pca1'] = pca.transform(df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no missing values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(df.drop('Month',axis= 1).corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking teh distribution \n",
    "df.total_distance.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df.total_distance ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fare_amount.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the outliers for non gaussian distributions\n",
    "def identify_outliers(data = df, column = 'total_distance'):\n",
    "    from numpy import percentile\n",
    "    # To indentify outliers in a Non Gaussian distribution we use 25 and 75 percentiles \n",
    "    q25, q75 = percentile(data[column], 25) , percentile(df.total_distance, 75) \n",
    "    iqr = q75 - q25\n",
    "    # calculate the outlier cutoff - Lower and upper\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    #print(f'Upper {upper}, Lower {lower}')\n",
    "    # identify outliers\n",
    "    outliers = [x for x in data[column] if x < lower or x > upper]\n",
    "    output = ('Identified outliers: %d' % len(outliers))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fare_amount' , 'total_distance' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    print(col,identify_outliers(column = col)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['passenger_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no outliers in the column passenger count\n",
    "\n",
    "print(identify_outliers(data = data , column = 'pickup_latitude')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['passenger_count'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does the number of passengers affect the fare?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 passengers \n",
    "df.groupby('passenger_count')['fare_amount'].mean().plot(kind='line')\n",
    "\n",
    "# The average fare amount is a little bit higher for 2 passengers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Does the date and time of pickup affect the fare?**\n",
    "- **Does the day of the week affect the fare?**\n",
    "- **Does the distance travelled affect the fare?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_col(data = df , group = 'passenger_count', target = 'fare_amount' , kind = 'bar' ):\n",
    "    var = data.groupby(group)[target].mean().sort_values(ascending = False)\n",
    "    return var.plot(kind = kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['passenger_count','Hours','Day' , 'morning_night' ]\n",
    "for group in cols:\n",
    "    print(group,target_col(group = group , kind='bar' ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is clear sign that hours si a feature with high impact in the fare amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that there is only one month\n",
    "df.Month.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does the distance travelled affect the fare?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Month',axis= 1).corr()['fare_amount'].sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection\n",
    "xgboost , Extra tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# XGBOOST method 1\n",
    "############\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "X = df.drop('fare_amount',axis=1 )\n",
    "y = df.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "\n",
    "X_train, X_valid , y_train , y_valid = train_test_split(X,y , test_size =0.3,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_valid)\n",
    "error = mse(y_valid,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we first train and then evaluate an XGBoost model on the entire training dataset and test datasets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.sort(model.feature_importances_)\n",
    "\n",
    "for thresh in thresholds:\n",
    "\t# select features using threshold\n",
    "\tselection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "\tselect_X_train = selection.transform(X_train)\n",
    "\t# train model\n",
    "\tselection_model = XGBRegressor()\n",
    "\tselection_model.fit(select_X_train, y_train)\n",
    "\t\n",
    "    # eval model\n",
    "\tselect_X_test = selection.transform(X_valid)\n",
    "\tpredictions = selection_model.predict(select_X_test)\n",
    "\terror = mse(y_valid, predictions)\n",
    "\tprint(\"Thresh=%.3f, n=%d, MSE: %.2f\" % (thresh, select_X_train.shape[1], error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see the highest error is with 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# XGBOOST method 2\n",
    "############\n",
    "\n",
    "# plot feature importance\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# fit model no training data\n",
    "model.fit(X, y)\n",
    "# plot feature importance\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Extra Tree Regressor \n",
    "############\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model_tree = ExtraTreesRegressor()\n",
    "model_tree.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_importances = pd.Series(model_tree.feature_importances_,index = X.columns )\n",
    "f_importances.nlargest(7).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings \n",
    "- The distance and fare class are the most important features based on Extra tree regressor and Xgboost feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling\n",
    "xgboost, random forest ,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate various models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR \n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "seed = 0  # Use the same seed through the exercises  \n",
    "models = []\n",
    "results = [] \n",
    "names = []\n",
    "\n",
    "models.append(('XGB', XGBRegressor(silent=True)))\n",
    "models.append(('RF', RandomForestRegressor()))\n",
    "models.append(('LSVR', LinearSVR()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result 1 - No scaling \n",
    "for model_name,model in models: \n",
    "    print(f'Now training... {model_name}')\n",
    "    kfold = KFold(n_splits = 5, random_state = seed , shuffle=True ) # change the seed\n",
    "    print(f'Calculating cv_results...')\n",
    "    cv_results = -1 * cross_val_score(model,X,y, cv = kfold,scoring='neg_mean_squared_error')\n",
    "    print(f'Appending to results...')\n",
    "    results.append(cv_results)\n",
    "    names.append(model_name)\n",
    "    msg = \"%s: %f (%f)\" % (model_name, cv_results.mean(), cv_results.std())\n",
    "    \n",
    "    print(f'Result {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X and y to make LSVR converge \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result 2 - Scaling - Here I pass the scaled version \n",
    "\n",
    "for model_name,model in models: \n",
    "    print(f'Now training... {model_name}')\n",
    "    kfold = KFold(n_splits = 5, random_state = seed , shuffle=True ) # change the seed\n",
    "    print(f'Calculating cv_results...')\n",
    "    \n",
    "    # Observe that here I will use the scaled version of X \n",
    "    cv_results = -1 * cross_val_score(model, X_scaled,y, cv = kfold,scoring='neg_mean_squared_error')\n",
    "    print(f'Appending to results...')\n",
    "    results.append(cv_results)\n",
    "    names.append(model_name)\n",
    "    msg = \"%s: %f (%f)\" % (model_name, cv_results.mean(), cv_results.std())\n",
    "    \n",
    "    print(f'Result {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()\n",
    "\n",
    "# We see the same results but LSVR needed to have scaled data as inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "- Add findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyper parameter tunning \n",
    "randomized, gaussian  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tune Random forest R and Xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a function to track how long time it takes to run the function\n",
    "\n",
    "from time import time \n",
    "\n",
    "def timer(func): \n",
    "    def wrapper(*args ,**kwargs): \n",
    "        t1 = time()\n",
    "        result = func(*args , **kwargs )\n",
    "        t2 = time\n",
    "        print(f'It took {t2-t1} for this function to run')   \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - Select the best n_estimators: \n",
    "@timer\n",
    "def get_score(n_estimators , cv = 5 ): \n",
    "    print(f'Now training: {n_estimators }')\n",
    "    model = RandomForestRegressor(n_estimators = n_estimators , random_state = seed)\n",
    "    print(f'Now obtaining the score for: {n_estimators }')\n",
    "    score = -1 * cross_val_score(model, X, y, cv = cv , scoring = 'neg_mean_squared_error' )\n",
    "    \n",
    "    return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training: 50\n",
      "Now obtaining the score for: 50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'builtin_function_or_method' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-35ac6591d6ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#for i in n_estimators:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-217f779c3e2e>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'It took {t2-t1} for this function to run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'builtin_function_or_method' and 'float'"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "n_estimators = [50, 100, 150] # \n",
    "get_score(50)\n",
    "\n",
    "#for i in n_estimators: \n",
    "    #results[i] = get_score(i)\n",
    "    \n",
    "\n",
    "# NOTE: Re run the loop with mean squared error . This is taking too long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "grid = RandomizedGridSearchCV(best_estimator = xgb, param_distributions = params , cv = 5, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:06:44] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:06:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:06:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:06:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:07:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:07:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:08:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:09:36] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:10:48] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:11:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:13:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:16:45] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:20:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:23:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:26:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:29:23] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:30:34] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:31:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# Standard params\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(random_state = seed  , silent=True)\n",
    "\n",
    "params = {\"learning_rate\": uniform(0, 1),\n",
    "              \"gamma\": uniform(0, 5),\n",
    "              \"max_depth\": range(1,50),\n",
    "              \"n_estimators\": range(1,300),\n",
    "              \"min_child_weight\": range(1,10)}\n",
    "\n",
    "random_grid =  RandomizedSearchCV(xgb ,param_distributions = params ,scoring ='neg_mean_squared_error', n_iter = 25)\n",
    "random_grid.fit(X,y)\n",
    "\n",
    "y_rs = np.maximum.accumulate(random_grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 params \n",
    "bds = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n",
    "        {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n",
    "        {'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)},\n",
    "        {'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 300)},\n",
    "        {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)}]\n",
    "\n",
    "# 2 Obtain the cv score- Optimization objective \n",
    "\n",
    "def cv_score(parameters):\n",
    "    parameters = parameters[0]\n",
    "    score = cross_val_score(\n",
    "                XGBRegressor(learning_rate=parameters[0],\n",
    "                              gamma=int(parameters[1]),\n",
    "                              max_depth=int(parameters[2]),\n",
    "                              n_estimators=int(parameters[3]),\n",
    "                              min_child_weight = parameters[4]), \n",
    "                X, y, scoring='neg_mean_squared_error').mean()\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "# 3 Obtain the cv score\n",
    "\n",
    "optimizer = BayesianOptimization(f=cv_score, \n",
    "                                 domain=bds,\n",
    "                                 model_type='GP',\n",
    "                                 acquisition_type ='EI',\n",
    "                                 acquisition_jitter = 0.05,\n",
    "                                 exact_feval=True, \n",
    "                                 maximize=True)\n",
    "\n",
    "# Only 20 iterations because we have 5 initial random points\n",
    "optimizer.run_optimization(max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bo = np.maximum.accumulate(-optimizer.Y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random search neg. MSE = {y_rs[-1]:.2f}')\n",
    "print(f'Bayesian optimization neg. MSE = {y_bo[-1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Artificial Neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import LeakyRelu,PRelu ,ELU\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train , valid splits \n",
    "X_train , X_valid , y_test , y_valid= train_test_split(X,y, test_size = 0.3, random_state= seed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# We will use normal as initialization technique \n",
    "model.add(Dense(128, kernel_initializer = 'normal', input_dm = X_train.shape[1] , activation = 'relu' )) # I will use relu activation function for all the nodes\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(256, kernel_initializer = 'normal',activation = 'relu' )) # we could also use leaky relu but we would need to tune the model\n",
    "model.add(Dense(256, kernel_initializer = 'normal',activation = 'relu' )) \n",
    "model.add(Dense(256, kernel_initializer = 'normal',activation = 'relu' )) \n",
    "\n",
    "# Output Layer \n",
    "model.add(Dense(1, kernel_initializer = 'normal',activation = 'linear' )) # we use linear fo regression problems  \n",
    "\n",
    "# Compile\n",
    "model.compile(loss = 'mean_squared_error' , optimizer ='adam' , metric=['mean_squared_error'] )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_valid, y_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tabe/Desktop/Courses/Data-Science /Test-Projects/Machine-Learning-Projects/New York City Taxi Fare Prediction\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (3/3), done.\n",
      "From https://github.com/tabers77/Machine-Learning-Projects\n",
      "   d3df58a..64a9e7d  main       -> origin/main\n",
      "Updating d3df58a..64a9e7d\n",
      "Fast-forward\n",
      " README.md | 1 \u001b[32m+\u001b[m\n",
      " 1 file changed, 1 insertion(+)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# 2 Pull request\n",
    "! git pull\n",
    "\n",
    "# 3\n",
    "now = datetime.datetime.now()\n",
    "commit_message = \"Last run on \" + str(now)\n",
    "! cd '/Users/Tabe/Desktop/Courses/Data-Science /Test-Projects/Machine-Learning-Projects/New York City Taxi Fare Prediction'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 924080e] Last run on 2020-11-17 08:35:11.630611\n",
      " 2 files changed, 110 insertions(+), 22 deletions(-)\n",
      "Enumerating objects: 9, done.\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 4 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 3.78 KiB | 645.00 KiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/tabers77/Machine-Learning-Projects.git\n",
      "   7f6d215..924080e  main -> main\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "! echo $commit_message > commit_message.txt\n",
    "! git add . \n",
    "! git commit -F commit_message.txt\n",
    "! git push\n",
    "#! git push origin master # here I choose either master or branch \n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
